\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e}
\usepackage[backend=bibtex,style=ieee,natbib=true]{biblatex}
\usepackage{graphicx}
\usepackage{pifont}  % for the checkmark/crossmark
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{xurl}

\addbibresource{main.bib}

\begin{document}

\title{
    COMPSCI 402 --- Artificial Intelligence\\
    Final Report\\
    The Development and Outlook of AI Techniques in The Field of
    Practical Federated Learning on Mobile Devices
}

\author{
    \IEEEauthorblockN{Sichang He}\\
    sichang.he@dukekunshan.edu.cn
}

\maketitle

\begin{abstract}
% TODO: Insert a very brief paragraph to summarize your essay
\end{abstract}

\section{Introduction}

% Briefly introduce your understanding of AI,

Artificial intelligence (AI) refers to
artificial machineries that resembles human intelligence,
either in terms of their functionality or behavior.
These machineries, or intelligent agents,
are not limited to,
but are commonly implemented in the form of computer software programs
running on general-purpose computer hardware.

Machine learning (ML), a significant subfield in AI,
focuses on the study of intelligent agents that can
adapt to previously unseen situations based on historical data.
During this adaptation process,
an intelligent agent analyzes data from the past to
prepare its abilities,
also known as ``learning'', or ``training''.
The data used is respectively referred to as ``training data''.
Upon completion of training,
the agent would be able to make predictions or inferences,
even when facing previously unobserved situations.
To implement ML, the common approach is to build models,
programs that implement mathematical functions.
These models are then trained by adjusting their function parameters.

% provide an overview of the application of the AI in your research area.
% For example, you can discuss the current development trend and
% provide a roadmap of the development of AI techniques in your research area.

Federated learning (FL), first proposed in 2016, is an ML technique that
allows distributed intelligent agents to collaborately train a model using
their local data~\cite{mcmahan2017communication,yang2019federated}.
These private data are kept local,
therefore FL is suitable for scenarios where
the training data cannot be shared to a central agent due to
privacy restrictions.

The growing demand for ML model training in various business sectors,
the rising awareness of data privacy concerns among individuals,
and the improvements in government laws have spurred
the popularity of FL applications.
It is a compelling option that enables companies to train ML models using
user data without violating some of the privacy regulations.

FL finds relevance in situations where
individual intelligent agents' private data are at the core of an ML problem,
a common scenario in FL applications on mobile devices.
Early practical applications of FL,
as proposed in~\cite{bonawitz2019towards},
included item selection and ranking,
personalized content recommendations,
and next-word prediction for smart keyboard on mobile devices.
In all these applications,
the private data from the users' interactions with
the keyboard are essential for the problems.
Since then, a significant portion of practical applications of FL have been
revolved around mobile devices,
including vision-based product quality inspection~\cite{bharti2022edge} using
an iOS application and
SMS spam classification on Android devices~\cite{sriraman2022device}.
The wide reach to private data from personal mobile devices makes
them the ideal participants for many FL applications.

Despite the theoretical advantages of FL on mobile devices,
the practical deployment of its applications are still limited.
The concept of FL was introduces as early as 2016~\cite{mcmahan2017communication},
and the its first effective and widely recognized application
was announced in 2019~\cite{bonawitz2019towards},
yet, even in 2023, practical FL applications on mobile devices remain scarce.
This scarcity can be attributed to multiple challenges practical FL encounters,
including privacy and security concerns,
training efficiency and performance issues,
and heterogeneity problems~\cite{wen2023survey}.

To approach practical FL on mobile devices,
we first present a general overview of the FL procedure in
Section~\ref{sec:general-procedure}.
Then, we analyze and address three key challenges encountered in
FL on mobile devices—scheduling (Section~\ref{sec:scheduling}),
communication (Section~\ref{sec:communication}), and
on-device training (Section~\ref{sec:on-device}).
Finally, we discuss additional challenges and
provide a potential roadmap for practical FL on mobile devices in
Section~\ref{sec:discussion}.

\section{The General Procedure of Federated Learning on Mobile Devices}

\label{sec:general-procedure}

\begin{algorithm}
  \caption{The General Procedure of Federated Learning}
  \label{algo:general-procedure}
  \ForEach{Training iteration}{
    Distribute the global model to all clients\;
    \ForEach{Client}{
      Train the local model using local training data\;
      Send the trained local model back to the central server\;
    }
    The central server aggregates all local models into a new global model\;
  }
\end{algorithm}

Generally, FL involves a distributed system that
consists of two parties of intelligent agents—the
clients and the central server.
The process typically unfolds through a series of training iterations,
with four phases per iteration,
as illustrated in Algorithm~\ref{algo:general-procedure}.
From the perspective of the ML models being trained,
in each iteration,
the central server orchestrates the distribution of the global model
to the clients.
Subsequently, the clients train the global model locally using
their individual local training data.
Following local training,
clients transmit their trained local models back to the central server.
Finally, these local models are aggregated to generate an updated global model,
and the process is repeated for successive iterations.
In the context of FL on mobile devices,
clients are typically mobile applications running on Android or iOS devices,
and the central server is a remote server on the Internet,
as depicted in Fig.~\ref{fig:general-fl}.

\begin{figure}
\centerline{
    \includegraphics[width=0.5\textwidth]{general-fl.png}
}
\caption{General Procedure of FL for Mobile Devices.}
\label{fig:general-fl}
\end{figure}

The specific implementations of FL on mobile devices exhibit variations,
but they all share a common foundation,
which we delve into in the following.

\subsection{The FedAvg Algorithm}

\newcommand{\FedAvg}{\texttt{FedAvg}}

The pioneering FL algorithm, \FedAvg{},
was presented with FL itself~\cite{mcmahan2017communication}.
It also stands as the most typical FL algorithm,
with many of its aspects regarded as the standard FL procedures.
Numerous other FL scheduling strategies are slightly modified version of
\FedAvg{} and preserves its core principles.

In \FedAvg{},
we assume a fixed number of $K$ clients,
each client with $k$ a fixed partition $P_k$ of
the overall training data ($k \in \{1, 2, \dots, K\}$) as their local data.
For efficiency,
in each iteration $t$ of the training process,
the central server randomly samples $C$ clients for local training.
Each selected client $k$ receives the parameters $w^{(t)}$ of
the latest global model from the server,
and then train the model locally using their local data $P_k$
to produce a new local model.
The objective of this local training process is to minimize the loss $L$ of
the model with parameters $w_k$ for partition $P_k$:
\begin{equation}
    \min_{w_k} L(P_k;w_k).
\end{equation}
The local training is scheduled for a fixed number of $E$ epochs,
ultimately yielding an updated set of weights $w_k^{(t+1)}$ for
each selected client $k$.

To aggregate the local models into a global model
optimized for the entire training dataset $\bigcup_k P_k$,
the objective for the global model entails
a weighted average of local losses:
\begin{equation}
    \min_{w} \frac{\sum_k |P_k|L(P_k;w)}{\sum_k |P_k|}
\end{equation}
where $|P_k|$ is the size of partition $P_k$.
To approach this objective,
\FedAvg{} aggregates the parameters of local models by
computing their weighted average:
\begin{equation}
    w^{(t+1)}=\sum_k \frac{|P_k|}{\sum_k |P_k|}w_k^{(t+1)}.
\end{equation}

The above iteration step is repeated multiple times until
model convergence or experiment termination.

\FedAvg{} has been demonstrated to be effective and practical
in a variety of experimental scenarios~\cite{mcmahan2017communication}.
Specifically, it was benchmarked against \verb|FedSGD|\cite{chen2016revisiting},
an earlier algorithm developed for distributed ML,
in data center settings~\cite{bonawitz2019towards}.

\subsection{The Specific Challenges of Practical Federated Learning on
Mobile Devices in the Context of FedAvg}

FL on mobile devices introduces distinct challenges compared to
FL in data center settings.
Mobile devices are a diverse set of intelligent agent hosts compared to
servers in data centers.
They exhibit a high degree of heterogeneity,
encompassing various hardware and software configurations.
Thus, applying \FedAvg{} to FL on mobile devices in practical scenarios
presents unique difficulties.

\subsubsection{Scheduling}

FL on mobile devices require tailored scheduling strategies.
\FedAvg{} waits for each client to complete its training task in
each iteration.
However, mobile clients differ greatly in their computational capabilities,
leading to variations in training time.
A small number of clients may lag significantly behind others,
ultimately becoming the bottleneck for the overall training process—a
phenomenon known as
``the straggler problem''~\cite{chen2020asynchronous,zheng2017asynchronous}.

The straggler problem significantly decrease the training efficiency in FL.
While strategies like backup workers have been employed to
mitigate this problem in traditional settings~\cite{chen2016revisiting},
such method is not suitable in the context of FL on mobile devices because
each client typically holds a distinct set of data.
Novel scheduling strategies are developed to
enhance the efficiency of FL on mobile devices.

\subsubsection{Communication}

FL on mobile devices faces significant challenges in effective communication.
In \FedAvg{}, all model parameters are transmitted between
clients and the central server.
In FL on mobile devices,
clients primarily connect to the server via wireless networks,
which constrains both transmission bandwidth and reliability.
Various techniques are adopted to reduce data transmission volumes
and enhance the reliability of the transmissions.

\subsubsection{On-Device Training}

The final obstacle that prevents practical FL on mobile devices is
on-device training.
To start with, training ML models on mobile devices has been
a cutting-edge technology and
is generally not well-supported across mobile platforms.
Additionally,
while \FedAvg{} assumes uniformity in model parameter formats known to
all participating agents,
mobile on-device training typically employs platform-specific ML frameworks
that diverge from those on the server.
This parameter misalignment challenge is compounded when
mobile clients operate on distinct platforms with
different ML frameworks.
Addressing the on-device training issue entails
experimentation with various ML frameworks,
and in some cases,
the development of custom training implementations tailored for
specific FL frameworks

\section{Customizations in Federated Learning Scheduling Strategies for
    Mobile Devices
}

\label{sec:scheduling}

The inherent heterogeneity of mobile devices necessitates
the adoption of tailored scheduling strategies for FL.
With the conventional FL setup like \FedAvg{} applied,
the stragglers would dictate the training efficiency of the whole system.
Moreover, the data heterogeneity among the clients also poses a challenge in
model convergence~\cite{zhang2022fedada}.
Therefore, customizations must be applied on scheduling strategies.
These customizations need to be first developed,
then tested and implemented in practice under FL frameworks.

\subsection{Developing Scheduling Strategies}

The typical settings for the original \FedAvg{} can be modified in
various ways to cater to diverse FL requirements.
For example,
client selection can be changed to a function based on
each client's properties instead of being random,
and the number of clients sampled per iteration can be made variable.
Similarly, alternative aggregation methods for local parameters
may be employed.

Various strategies have been proposed to address the straggler problem.
One approach involves implementing a timeout mechanism to exclude
straggler devices~\cite{bonawitz2019towards}.
However, this approach is not suitable for FL on mobile devices because
the inherent low performance of some devices would render
their training data permanently excluded and wasted.

\newcommand{\FedProx}{\texttt{FedProx}}
\newcommand{\TFF}{TensorFlow Federated~\cite{tff}}

Many strategies to mitigate the heterogeneity problem rely on
synchronous aggregation.
For instance,
\FedProx{}~\cite{li2020federated}
allows clients to train for different numbers of local epochs according to
their computational capabilities.
In heterogeneous settings,
\FedProx{} has demonstrated superior efficiency compared to \FedAvg{} .
It has been widely supported in FL frameworks designed for simulations and
benchmarks, such as
\TFF{},
Syft~\cite{ryffel2018generic,Ziller2021,hall2021syft}, and
LEAF~\cite{caldas2018leaf}.
Other similar optimizations such as
\verb|ef-signSGD|~\cite{karimireddy2019error},
\verb|FedAdam|~\cite{reddi2020adaptive},
and~\cite{luo2021cost} have also been explored.

\newcommand{\FedML}{FedML~\cite{he2020fedml}}
\newcommand{\Florida}{Project Florida~\cite{madrigal2023project}}

Alternatively,
as opposed to strict iterations,
the training process can adopt an
asynchronous approach~\cite{chilimbi2014project,zhu2022online,huba2022papaya}.
Asynchronous aggregation has been adopted in
FL frameworks such as \FedML{} and \Florida{}.
Semi-asynchronous FL techniques have also been explored,
such as in~\cite{sun2022fedsea}.

\subsection{Testing Scheduling Strategies}

Numerous FL frameworks have been developed for
conducting production-level FL simulations and
experimentation with scheduling strategies.
Google's \TFF{} is widely adopted as infrastructure for simulation because of
its comprehensive features.
PaddleFL\footnote{\url{
    https://github.com/PaddlePaddle/PaddleFL
}.} in PaddlePaddle~\cite{ma2019paddlepaddle} by Baidu,
FATE~\cite{liu2021fate} by Tencent, and
OpenFL~\cite{patrick2022openfl} by Intel
serve similar purpose,
offering environments for production-grade FL simulations.
Additionally,
FL benchmarks such as FedScale~\cite{lai2022fedscale} are instrumental in
comparing and identifying scheduling strategies with superior performance.

\subsection{Implementing Custom Scheduling Strategies in Practice}

\newcommand{\Flower}{Flower~\cite{beutel2020flower}}

In FL frameworks such as \FedML{} and \Flower{},
the customization of scheduling strategies on the client side is
left entirely to the users.
From a programming perspective,
these frameworks provide interfaces,
also known as abstract classes,
for custom clients to implement.
These interfaces typically encompasses methods for
local model training,
model parameter retrieval and modification,
and message handling\footnote{\url{
    https://github.com/FedML-AI/FedML/blob/9aeb0c097efc9ea7037cfe24499c3d61c81c4dca/android/fedmlsdk/src/main/java/ai/fedml/edge/FedEdgeApi.java
}.}\footnote{\url{
    https://github.com/adap/flower/blob/4df413f8a1696f643fdde27bf7bac8c33b623f56/src/kotlin/flwr/src/main/java/dev/flower/android/Client.kt
}.}\footnote{\url{
    https://github.com/adap/flower/blob/4df413f8a1696f643fdde27bf7bac8c33b623f56/src/swift/flwr/Sources/Flower/Client/Client.swift
}.}.
These interfaces greatly increases the flexibility for the users to
design and implement custom scheduling strategies,
but the responsibility of implementing these strategies can
sometimes be burdening,
especially considering that the mobile clients often need to
be implemented in Java or Swift.

On the server side,
while the customizations are typically more accessible with
open-source frameworks,
the emergence of FL as a service (FLaaS)
introduces complexity.
These FLaaS providers manage proprietary servers,
often accessible to the users through opaque APIs or
graphical user interfaces (GUIs).
For instance, \FedML{}
offers a set of Python APIs for server customization
and enables users to upload their server implementations via a web GUI.
\Florida{}, another production-ready FLaaS by Microsoft,
supports the uploading of Python scripts or .NET executables for
server aggregation functions.
While these FLaaS solutions also provide web GUI for convenience,
their customizability is often limited.

\subsection{Communication Implementations Among Mobile Clients and the Server}

\label{sec:communication}

The restricted network connectivity on mobile devices necessitates
efficient and reliable communication in FL.
To reduce the amount of data transmitted,
technique such as only transmitting the difference of the model parameters
have long been applied\footnote{\url{
    https://ai.googleblog.com/2017/04/federated-learning-collaborative.html
}.}.
Furthermore,
Frameworks like Hermes~\cite{li2021hermes} have been developed to
reduce the amount of transmitted model parameter data by
leveraging sub-networks.
However, advanced communication techniques are usually difficult to
implement on mobile devices because
mobile development environments are much less friendly than
the ones researchers interface in theoretical research.

In practice,
Remote Procedure Call (RPC) is the standard communication method for
practical FL.
This is likely due to the flexibility it brings to the FL process.
Under an RPC framework,
clients and the server can transfer instructions for the other side to execute.
Therefore, RPC allows for straightforward customization of
FL schedule strategies.

\FedML{} uses an abstract communication layer on top of
the concrete implementations.
These implementations either use the message passing interface
(MPI)\footnote{\url{
    https://www.mpi-forum.org/
}.} over HTTP,
or uses the MQTT protocol\footnote{\url{
    https://mqtt.org/mqtt-specification/
}.}.

The gRPC Remote Procedure Call\footnote{\url{
    https://grpc.io/
}.} is another popular choice for FL communication.
It is used by \TFF{} and
OpenFL~\cite{patrick2022openfl} for decentralized simulation.
Flower uses gRPC for the standard implementation of
its communication protocol, the Flower Protocol~\cite{beutel2020flower}.
And, \Florida{} allows users to choose between gRPC and REST.

gRPC accompanied by protocol buffers (ProtoBuf)\footnote{\url{
    https://protobuf.dev/
}.} is desirable for communication between FL mobile participants for
several reasons.
ProtoBuf is a compact binary representation for data structures,
allowing for efficient transmission~\cite{popic2016performance},
therefore it is suitable for mobile devices that
are often under restricted network conditions.
gRPC and ProtoBuf are language-agnostic and
provide a wide range of programming language support,
facilitating its adoption on different platforms~\cite{araujo2020performance}.
It is also based on HTTP/2,
therefore it can usually bypass mobile devices' restricted firewalls and
pass through proxies~\cite{araujo2020performance}.
Being connection-based and bidirectional,
gRPC also allows the server to recognize
which clients have lost their connections and
push instructions to the clients.

Syft uses WebSockets for communication between its network-based workers and
the server~\cite{Ziller2021}.

\section{On-Device Training for Federated Learning on Mobile Devices}

\label{sec:on-device}

Most FL frameworks cannot support practical FL on mobile devices due to
the lack of on-device training support.
These examples include simulation-oriented frameworks such as
\TFF{}~\cite{kholod2020open} and
LEAF~\cite{caldas2018leaf}.
PaddlePaddle~\cite{ma2019paddlepaddle}
only supports inference on Android.

Most FL frameworks that support on-device training on mobile devices do not
cover all platforms,
or lacks support for hardware acceleration,
as shown in Table~\ref{tab:on-device}.
As an exception,
\Flower{} leaves the on-device training implementation to
its users,
but provides example Android and iOS implementations\footnote{\url{
    https://github.com/adap/flower/tree/main/examples/android-kotlin
}.}\footnote{\url{
    https://github.com/adap/flower/tree/main/examples/ios
}}.


\begin{table}
\caption{Mobile On-Device Training Functionalities of
    Federated Learning Frameworks.
}
\begin{center}
\begin{tabular}{cccc}
    Functionality&Android&iOS&Hardware Acceleration\\
    \hline
    TensorFlow Federated&\xmark&\xmark&\xmark\\
    LEAF&\xmark&\xmark&\xmark\\
    PaddleFL&\xmark&\xmark&\xmark\\
    Flower&\cmark$^\mathrm{a}$&\cmark$^\mathrm{a}$&\cmark$^\mathrm{a}$\\
    FedML&\cmark&\xmark&\cmark\\
    Syft&\cmark&\cmark&\xmark\\
    FedScale&\cmark$^\mathrm{b}$&\xmark&\cmark\\
    \multicolumn{4}{l}{$^{\mathrm{a}}$Up to the user to implement.}\\
    \multicolumn{4}{l}{$^{\mathrm{b}}$Only in Termux.}
\end{tabular}
\label{tab:on-device}
\end{center}
\end{table}
\subsection{First-Party Mobile Machine Learning Frameworks}

Most FL systems utilize existing mobile ML frameworks to
support on-device training.
Google's TensorFlow Lite~\cite{tensorflow2015-whitepaper,abadi2016tensorflow}
for Android is
a popular choice due to its flexibility and
its maturity for on-device training.
For example, TensorFlow Lite is used by FLaaS frameworks such as~\cite{
    kourtellis2020flaas,katevas2022flaas}
for their Android support.
Flower's Android example also uses
TensorFlow Lite~\cite{beutel2020flower,mathur2021ondevice}.
In contrast, Apples's Core ML~\cite{coreml} is less popular because
it is much less user-friendly.
Core ML is only used by a few FL frameworks such as
\Flower{} for its iOS example,
and is more commonly used for specific applications
such as this vision-based quality inspector application~\cite{bharti2022edge}.

Unfortunately, adopting first-party ML frameworks causes difficulties when
the objective is the conduct FL that involve both Android and iOS devices.
TensorFlow Lite only supports Android and
Core ML only supports iOS,
resulting in different ML frameworks being used for each platform.
These two frameworks produces model parameters in different representations,
so it is difficult to aggregate these local models into a single global model.
For example, to use \Flower{} to conduct FL across Android and iOS devices,
we recently developed a tedious solution that involves
ProtoBuf manipulation~\cite{he2023fedkit}.
This limitation means that most FL frameworks using first-party ML frameworks
will likely have separate training support for Android and iOS.

\subsection{Third-Party Mobile Machine Learning Frameworks}

Third-party solutions for these mobile operating systems are less commonly used.
For example, \FedML{} uses
the MNN ML framework~\cite{jiang2020mnn,lv2022walle} by Alibaba for
its on-device training on Android.
MNN claims to support Android and iOS and promises lightweight binaries.
However, in my attempt to use MNN, I found that its documentation is
severely lacking, outdated, and confusing.
Also, despite the claim that MNN supports iOS,
FedML still has not provided an iOS SDK 15 months after they planned
it\footnote{\url{
    https://github.com/FedML-AI/FedML/tree/9aeb0c097efc9ea7037cfe24499c3d61c81c4dca/ios
}.}.

Notably, Syft~\cite{ryffel2018generic,Ziller2021,hall2021syft} developed
its custom on-device training implementations for
both Android and iOS\footnote{\url{
    https://blog.openmined.org/announcing-new-libraries-for-fl-on-web-and-mobile/
}.}.
By using a custom implementation,
Syft ensures that the representations of model parameters are
consistent across platforms,
therefore it supports FL settings that involve both Android and iOS devices.
However, its focus is on security and privacy,
therefore the implementations greatly sacrifice
performance~\cite{ryffel2018generic}.
Furthermore, these custom implementations can only utilize the CPU,
therefore they are far from being practical for real-world applications of
FL on mobile devices.
Unfortunately, development on
both implementations for Android and iOS,
KotlinSyft\footnote{\url{
    https://github.com/OpenMined/KotlinSyft
}.} and SwiftSyft\footnote{\url{
    https://github.com/OpenMined/SwiftSyft
}.} has been inactive for over two years,
indicating that these projects have been abandoned due to a lack of interest.

There are third-party on-device training solutions that are worth keeping an key on,
but the difficulty to interface hardware on different platforms is
still an major challenge.
ONNX Runtime~\cite{onnxruntime} by Microsoft aims to
unify the ML experience on all platforms by
supporting Open Neural Network Exchange (ONNX),
an open-source format to represent deep learning
models~\cite{ParedesdelRio2020}.
However, its implementations are currently only utilizing CPUs,
leaving out the massive performance improvements by the hardware acceleration
from GPUs and NPUs on the processors of modern mobile devices.
This limitation severely restricts its practical usage.

\subsection{Not-For-Mobile Solutions}

Some systems choose to adopt solutions that are not specifically designed for
mobile ML,
including web-based and UNIX-environment-based solutions.
For example,
TensorFlow.js is capable of running inside a browser on mobile devices or
a webview inside mobile applications~\cite{smilkov2019tensorflow}.
The usage of TensorFlow.js by~\cite{sriraman2022device} resulted in
difficulties in testing,
but~\cite{palanisamy2021spliteasy} shows the advantages of TensorFlow.js on iOS
by integrating it into a React Native application for split learning.
Another example is FedScale~\cite{lai2022fedscale},
which uses Termux, a UNIX console on Android,
to run TensorFlow directly.
FedScale's solution is only suitable for its benchmarking purposes because
it requires the Termux application to run and
cannot be embedded.

\subsection{Model Personalization}

Model personalization is another popular variant of FL.
While FL generally aims to train a global model,
in model personalization,
local models are adjusted using each user's data to
adapt to the each specific user.
Therefore, model personalization excels general FL when
facing high statistical heterogeneity among the users~\cite{kulkarni2020survey}.
Model personalization also reduces some restrictions and difficulties that
general FL faces.
For example,
we usually aim to update most or all layers in general FL,
but it is common to only update the last layer of neural networks in
transfer learning,
a type of model personalization.
By only updating the last layer,
model personalization can remain efficient even when
the neural network used is deep;
in contrast, the computational complexity in back propagation for
deep neural networks makes it unsuitable for general FL on mobile devices.
Therefore, general FL either requires smaller models to be used,
or partial updates,
or other techniques to be applied so it does not train the whole model.

% (You can use table to summarize the features of existing methods;
% or you can conduct the comparative study by
% testing some state-of-the-art methods on your selected dataset.)

\section{Discussion}

\label{sec:discussion}

% TODO: Provide an outlook on the development of AI technology in
% your research area based on your knowledge of your research area and
% your understanding of AI.
% You can discuss some open challenges and try to
% provide the corresponding potential solutions or
% discuss the potential research directions.

On-device training remains the most significant obstacle for
practical FL on mobile devices.
Although popular FL frameworks such as
PySyft\footnote{\url{
    https://github.com/OpenMined/PySyft/
}.}~\cite{ryffel2018generic,Ziller2021,hall2021syft}
have democratized FL research by providing tooling and
algorithms~\cite{sriraman2022device},
most of them lack comprehensive support for operations on mobile devices.
Among the necessary operations for FL on mobile devices,
on-device training is the most challenging one because
implementing it requires either depending on
platform-specific ML frameworks that are highly experimental (TensorFlow Lite)
or proprietary (Core ML),
or developing home-grown solutions.
Most FL frameworks choose to use platform-specific ML frameworks,
while custom training implementations such as
the Android and iOS implementations of Syft are limited to CPU.
As a result, it is usually difficult to conduct on-device FL
on mobile devices using existing FL frameworks.

Security remains a critical concern in FL.
Attacks to FL systems to reverse engineer the training data have been
demonstrated to be practical~\cite{sun2019really}.
Mechanism that increase anonymity and
reduce the risk of successful attacks have been proposed and implemented.
For example,
homomorphic encryption (HE)~\cite{wang2020homo},
secure multi-party computing (SMC)~\cite{bonawitz2016practical}, and
differential privacy
(DP)~\cite{dwork2006differential,geyer2017differentially} are
adopted by many production-level FL frameworks and
their effectiveness has been demonstrated.
However, the real-world application on mobile devices still faces a fundamental
issue of trust.
After the mobile applications obtain the training data,
there is no obvious way to verify whether the data are used for FL,
or are in fact sent to a central server.
Companies may well use FL to cover their direct data collection under the hood.

\printbibliography

% TODO: at least 10 references,
% 50\% references should be published within 5 years,
% blogs/website/news reports should be less than 10\%.

\end{document}
